<?xml version="1.0" encoding="utf-8"?>
<search> 
  
    
    <entry>
      <title>第五期特训营第五期直播</title>
      <link href="/2018/07/27/TWSchool-5/"/>
      <url>/2018/07/27/TWSchool-5/</url>
      <content type="html"><![CDATA[<h1 id="视频地址"><a href="#视频地址" class="headerlink" title="视频地址"></a>视频地址</h1><a id="more"></a><video id="video" controls preload="auto" poster="/imgs/TWSchool-3.png" style="width:100%">      <source id="mp4" src="https://s3.cn-north-1.amazonaws.com.cn/tws-courses-resource/live-video/TWSchool5.mp4" type="video/mp4">      <p>Your user agent does not support the HTML5 Video element.</p></video><h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><p><img src="/imgs/TWSchool5-1.png" alt="总结"></p>]]></content>
      
      <categories>
          
          <category> TWSchool </category>
          
      </categories>
      
      
        <tags>
            
            <tag> TWSchool </tag>
            
            <tag> javascript </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>第五期特训营第四期直播</title>
      <link href="/2018/07/24/TWSchool-4/"/>
      <url>/2018/07/24/TWSchool-4/</url>
      <content type="html"><![CDATA[<h1 id="视频地址"><a href="#视频地址" class="headerlink" title="视频地址"></a>视频地址</h1><a id="more"></a><video id="video" controls preload="auto" poster="/imgs/TWSchool-3.png" style="width:100%">      <source id="mp4" src="https://s3.cn-north-1.amazonaws.com.cn/tws-courses-resource/live-video/TWSchool4.mp4" type="video/mp4">      <p>Your user agent does not support the HTML5 Video element.</p></video><p><strong>NOTE: 怪异模式有的地方也称作混杂模式，标准模式有的地方也称作严格模式</strong></p><h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><ul><li><a href="https://developer.mozilla.org/zh-CN/docs/Web/API/Storage" target="_blank" rel="noopener">localStorage&amp;sessionStorage</a></li><li><a href="https://developer.mozilla.org/zh-CN/docs/Web/HTTP/Cookies" target="_blank" rel="noopener">cookie</a></li><li><a href="https://www.ibm.com/developerworks/cn/web/1310_shatao_quirks/" target="_blank" rel="noopener">怪异模式和标准模式</a></li></ul>]]></content>
      
      <categories>
          
          <category> TWSchool </category>
          
      </categories>
      
      
        <tags>
            
            <tag> TWSchool </tag>
            
            <tag> javascript </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>第五期特训营第三期直播</title>
      <link href="/2018/07/18/TWSchool-3/"/>
      <url>/2018/07/18/TWSchool-3/</url>
      <content type="html"><![CDATA[<h1 id="视频地址"><a href="#视频地址" class="headerlink" title="视频地址"></a>视频地址</h1><a id="more"></a><video id="video" controls preload="auto" poster="/imgs/TWSchool-3.png" style="width:100%">      <source id="mp4" src="https://s3.cn-north-1.amazonaws.com.cn/tws-courses-resource/live-video/TWSchool3.mp4" type="video/mp4">      <p>Your user agent does not support the HTML5 Video element.</p></video><h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><p><img src="/imgs/TWSchool3-1.JPG" alt="FullStackDeveloper"></p>]]></content>
      
      <categories>
          
          <category> TWSchool </category>
          
      </categories>
      
      
        <tags>
            
            <tag> TWSchool </tag>
            
            <tag> javascript </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>第五期特训营第二期直播</title>
      <link href="/2018/07/13/TWSchool-2/"/>
      <url>/2018/07/13/TWSchool-2/</url>
      <content type="html"><![CDATA[<h1 id="视频地址"><a href="#视频地址" class="headerlink" title="视频地址"></a>视频地址</h1><a id="more"></a><video id="video" controls preload="auto" poster="/imgs/TWSchool-3.png" style="width:100%">      <source id="mp4" src="https://s3.cn-north-1.amazonaws.com.cn/live-video/%E7%AC%AC%E4%BA%8C%E6%9C%9F%E7%9B%B4%E6%92%AD-720-full.mp4" type="video/mp4">      <p>Your user agent does not support the HTML5 Video element.</p></video><p><strong>NOTE:视频中提到的“提交前先Pull Master”是指提交到远程前，即“Pull master before PUSH”，不是“Pull master before COMMIT”，虽然同为提交，但是写成英文后就是两个截然不同的提交时机，这里手动修正一下，大家日后也注意用词的准确性，暂时来不及更新视频了，抱歉</strong></p><h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><h2 id="Git团队协作workflow"><a href="#Git团队协作workflow" class="headerlink" title="Git团队协作workflow"></a>Git团队协作workflow</h2><p><img src="/imgs/TWSchool2-1.png" alt="workflow"></p><h1 id="课后作业"><a href="#课后作业" class="headerlink" title="课后作业"></a>课后作业</h1><h2 id="代码整洁练习"><a href="#代码整洁练习" class="headerlink" title="代码整洁练习"></a>代码整洁练习</h2><p><img src="/imgs/TWSchool2-2.png" alt="clean code"></p><h2 id="Git团队协作"><a href="#Git团队协作" class="headerlink" title="Git团队协作"></a>Git团队协作</h2><p>初始readme样例：<br><img src="/imgs/TWSchool2-3.png" alt="团队协作"></p><p>要求：</p><ul><li>指定一位同学做master，建立仓库、拉协作者、初始化仓库（用markdown实现上图readme），然后写上第一个自我介绍和第一句话/段落。</li><li>其他同学以协作者身份，陆续将自我介绍和一句话补全，注意不要删除前面同学的内容，拼手速，我们共有7名同学，第7名同学要给故事进行收尾。</li></ul><h1 id="直播讨论及答疑记录"><a href="#直播讨论及答疑记录" class="headerlink" title="直播讨论及答疑记录"></a>直播讨论及答疑记录</h1><h2 id="关于代码整洁的练习"><a href="#关于代码整洁的练习" class="headerlink" title="关于代码整洁的练习"></a>关于代码整洁的练习</h2><ol><li><p>同学：第三第四行有空格，不规范，第五行let前面有“//”</p><p> 老师：空格在日常写代码是有要求的，一般变量和函数之间是有一个空格。函数与函数间也是有空格的。这次题目是一些代码块，空格这个先不做要求哈。</p></li><li><p>同学：注释代码没删除；第二个函数名应该用小驼峰；第一个变量命名也应该用小驼峰<br> 老师：嗯，变量一般情况下是采用小驼峰的，如果这个数字在代码中是不需要改变的情况下就说明它是常量，常量一般都需要全大写，如果是变量的话就是需要采用小驼峰的。</p></li><li><p>同学：最后两个class啥问题啊，Alpaca属于animal，所以命名最好统一大小写？<br> 老师：后面两个class表示这是一个类。后面的animal是类名称，类名称的话一般是采取什么样的命名方式哩？<br> 同学A：用大驼峰法 ！<br> 同学B：类的命名应为大驼峰，首字母应大写，animal不对</p></li><li><p>同学：数组的变量名Artists应用小驼峰，变量名一般用小驼峰<br> 老师：是的。</p></li><li><p>同学A：什么叫类的命名？<br> 同学B：就是给一个类型取名字。<br> 老师：类是对现实生活中一类具有共同特征的事物的抽象，例如我们人可以属于一个类，我们有共同的属性，姓名，性别，年龄，还有共同的方法 吃饭，走路，跑，等等。这些都是在这个人的类里面包含的。</p></li><li><p>同学：第二个funtion有什么问题吗？<br> 老师A：第二个funtion用下划线其实也是可以接受的，但是一般情况下 他的命名最好采用小驼峰<br> 老师B：整个项目里的代码风格要统一，如果function选择下划线，那就都用下划线，如果用小驼峰，就都用小驼峰</p></li><li><p>正确答案：<br><img src="/imgs/TWSchool2-4.jpg" alt="Answer"></p></li><li><p>老师A：大家很多地方其实都回答正确的，很棒。前面两个数字 7和30 这里默认他们是常量（值是不变的），常量的命名一般采用全大写，多个单词的话，用下划线连接。<br> 老师B：我补充一点，就是大家以后学了js以后，尽量不要用var，用let和const像上面的这个代码，所有的var都可以换成const。</p></li></ol>]]></content>
      
      <categories>
          
          <category> TWSchool </category>
          
      </categories>
      
      
        <tags>
            
            <tag> TWSchool </tag>
            
            <tag> javascript </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>第五期特训营第一期直播</title>
      <link href="/2018/07/11/TWSchool/"/>
      <url>/2018/07/11/TWSchool/</url>
      <content type="html"><![CDATA[<h1 id="视频地址"><a href="#视频地址" class="headerlink" title="视频地址"></a>视频地址</h1><a id="more"></a><video id="video" controls preload="auto" poster="/imgs/TWSchool-3.png" style="width:100%">      <source id="mp4" src="https://s3.cn-north-1.amazonaws.com.cn/live-video/%E7%AC%AC%E4%B8%80%E6%9C%9F%E7%9B%B4%E6%92%AD-720.mp4" type="video/mp4">      <p>Your user agent does not support the HTML5 Video element.</p></video><h1 id="工具下载地址"><a href="#工具下载地址" class="headerlink" title="工具下载地址"></a>工具下载地址</h1><h2 id="Mac"><a href="#Mac" class="headerlink" title="Mac"></a>Mac</h2><ul><li><a href="https://www.alfredapp.com" target="_blank" rel="noopener">Alfred</a></li><li><a href="https://www.mediaatelier.com/CheatSheet/" target="_blank" rel="noopener">CheatSheet</a></li><li><a href="https://brew.sh" target="_blank" rel="noopener">Homebrew下载</a>和<a href="https://www.jianshu.com/p/c2bdc1997a9d" target="_blank" rel="noopener">使用说明</a></li><li><a href="https://github.com/robbyrussell/oh-my-zsh/wiki/Installing-ZSH" target="_blank" rel="noopener">zsh下载地址</a>和<a href="https://ohmyz.sh" target="_blank" rel="noopener">oh my zsh下载地址</a>和<a href="https://github.com/robbyrussell/oh-my-zsh/wiki/Themes" target="_blank" rel="noopener">zsh主题</a></li></ul><h2 id="Windows"><a href="#Windows" class="headerlink" title="Windows"></a>Windows</h2><ul><li><a href="https://www.jianshu.com/p/8fd212f5c01f" target="_blank" rel="noopener">Wox</a></li></ul><h1 id="集成开发环境"><a href="#集成开发环境" class="headerlink" title="集成开发环境"></a>集成开发环境</h1><ul><li><a href="https://www.jetbrains.com/webstorm/" target="_blank" rel="noopener">webstorm下载地址</a></li><li><a href="http://idea.lanyus.com" target="_blank" rel="noopener">webstorm破解地址</a></li><li>webstorm常用快捷键<br><img src="/imgs/TWSchool-1.png" alt="Mac"><br><img src="/imgs/TWSchool-2.png" alt="Windows"></li></ul>]]></content>
      
      <categories>
          
          <category> TWSchool </category>
          
      </categories>
      
      
        <tags>
            
            <tag> TWSchool </tag>
            
            <tag> javascript </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>模式识别与分析——贝叶斯决策理论（理论部分）</title>
      <link href="/2018/06/30/MIT-PatternRecognition3/"/>
      <url>/2018/06/30/MIT-PatternRecognition3/</url>
      <content type="html"><![CDATA[<h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><p>通常来讲，在解决一个分类问题时，我们的思维过程可以分为下面这三个部分：</p><p><strong>Measurement Space —-&gt; Feature Space —-&gt; Decision Space</strong></p><p>先思考如何测量衡量（可以理解为抽象这个问题并通过某种方式得到相应的数据），然后根据测量得到的结果提取特征，最后作出决策，有时Measurement Space和Feature Space是和在一起的。比如识别“B”和“8”，可能立刻马上想到的最简单的方法是：把“B”和“8”放入一个长方形，观察可以得到，“B”的左边竖线是平行于长方形的左边框的，而“8”不是，所以选取长方形左边框上一系列点，过这些做垂线段，找到与B/8的左边的交点，测量这一些列垂线段的长度（This is Measurement Space），如果长度几乎一样（This is Feature Space），则为B（This is Decision Space），否则为8。<br><a id="more"></a><br>如果给出了下面两个cases：</p><ol><li>条件概率密度函数和先验概率——measurement space &amp; feature space</li><li>训练所需的样本点——where we get our decision space</li></ol><p>那么就可以考虑使用贝叶斯方法做出分类决策。</p><p>贝叶斯决策理论（Bayesian decision theory）是一种根据概率进行决策的理论，在模式识别中，将分类当作决策进行预测。</p><h1 id="贝叶斯决策规则（简单版）"><a href="#贝叶斯决策规则（简单版）" class="headerlink" title="贝叶斯决策规则（简单版）"></a>贝叶斯决策规则（简单版）</h1><ul><li>M个类</li><li>给出（算出）类条件密度函数（class conditional density function）<script type="math/tex; mode=display">p(x|\omega_1), p(x|\omega_2), \dots , p(x|\omega_M), x \in R^n</script></li><li>给出（算出）先验概率$P(\omega_1), P(\omega_2), \dots, P(\omega_M)$<script type="math/tex; mode=display">0<P(\omega_i)<1, \forall i = 1,2, \dots, M</script><script type="math/tex; mode=display">\sum_{i=1}^MP(\omega_i)=1</script></li><li>如果$p(x|\omega_i)P(\omega_i)\geq p(x|\omega_j)P(\omega_j), \forall j \neq i$， 则认为$x$是$\omega_i$类，为什么呢？后面解释</li><li>最好的决策是：最小化错分类的概率，<strong>前提是所有错误的权重（代价）相等，否则此条不适用。</strong></li></ul><p><strong>NOTE:</strong><br>条件概率用小写p表示，先验概率用大写P表示。</p><h2 id="结合实例理解"><a href="#结合实例理解" class="headerlink" title="结合实例理解"></a>结合实例理解</h2><h3 id="理解公式"><a href="#理解公式" class="headerlink" title="理解公式"></a>理解公式</h3><p>我们用一个分类两种鱼的例子来说明贝叶斯规则以及它的合理性。</p><p>假设有两种鱼sea bass和salmon，随机取一条鱼，我们会如何猜测呢？在什么都不知道的情况下，我们当然会随机猜测，但是如果此时给你一份统计数据，告诉你这片海域60%是sea bass，40%是salmon，此时你会怎么猜测呢？当然猜sea bass了。其实这里的60%和40%就是先验概率，用大写字母P表示。在这种情况下，我们的决策规则是：<script type="math/tex">if\ P(sea \ bass)>P(salmon),\ then\ sea\ bass, \ otherwise \ salmon</script></p><p>猜一条鱼的时候，我们可以采用上面的方式猜，但是如果接下来的100条鱼都让你猜测，你还会用同样的决策规则吗？显然不会。那么有没有什么更聪明的方法呢？有。</p><p>我们可以探索更多的特征来辅助决策，增加我们决策的依据。</p><p>假如有人给我们提供了sea bass和salmon的光泽度密度函数，光泽度我们用$x$来表示，即给我们提供了$p(x|sea\ bass)$和$p(x|salmon)$。</p><p>现有一条鱼，经过测量，光泽度是5，那么我们怎么猜呢？我们要猜$p(sea\ bass|x=5)$和$p(salmon|x=5)$的概率，谁大就猜谁，这就是我们的决策策略（有同学会有疑问，怎么这个决策策略好像和第二部分写的不太一样？没关系，一会儿解答）。</p><p>ok我们现在问题抽象的已经很明确了——要计算$p(sea\ bass|x=5)$和$p(salmon|x=5)$的概率。想想我们已知的有哪些？首先我们知道先验概率：$P(sea\ bass)$和$P(salmon)$, 其次我们知道每种鱼的光泽度分布：$p(x|sea\ bass)$和$p(x|salmon)$。</p><p>ok，以计算$p(sea bass|x)$为例：</p><ul><li><p>(1): $p(sea\ bass|x) = p(sea\ bass, x) / p(x)$</p></li><li><p>(2): $p(x|sea\ bass) = p(x,sea\ bass) / P(sea\ bass)$</p></li><li><p>(3): 由(2)得：<script type="math/tex">p(sea\ bass\ ,\ x) =  p(x|sea\ bass) * P(sea\ bass)</script></p></li><li><p>(4): 把(3)带入(1)得：<script type="math/tex">p(sea\ bass|x) = p(x|sea\ bass) * P(sea\ bass) / p(x)</script></p></li></ul><p>使用上述方法就可以计算出p(sea bass|x)的概率，p(sea bass|x)同理，如此就可以给出x=5时候的猜测。</p><p>上述计算公式(4)就是<strong>贝叶斯公式</strong>，通用写法为：</p><script type="math/tex; mode=display">P\left(\omega_{j} | x \right) = \frac{p\left(x|\omega_{j}\right)P\left(\omega_{j}\right)}{p\left(x\right)}</script><p>其中$\omega$代表某个类，$x$代表某个特征，在这个二分类问题中：</p><script type="math/tex; mode=display">p(x)=\sum_{j=1}^2p(x|\omega_j)P(\omega_j)</script><p>看这个公式，所有类里，$p(x)$是一样的，所以我们只要比较$p(x|\omega_j)P(\omega_j)$的大小就可以对不对，这就是我们第二部分写的贝叶斯决策规则的由来。</p><p>贝叶斯公式也可以用英文来描述：</p><script type="math/tex; mode=display">posterior = \dfrac {likelihood*prior}{evidence}</script><p>所以贝叶斯公式将先验概率转换成了后验概率，式子中的evidence可以当作一个比例因子（scale factor），来确保posterior的和为1.</p><h3 id="理解错误（代价）"><a href="#理解错误（代价）" class="headerlink" title="理解错误（代价）"></a>理解错误（代价）</h3><p>依然是上例，如果我们猜是sea bass，可想而知，我们的错误率是$p(salmon|x)$,否则错误率是$p(sea \ bass|x)$。用公式表达为</p><script type="math/tex; mode=display">P(error | x)=\begin{cases}P(salmon|x), &\text{if we decide sea bass} &\\P(sea \ bass|x), &\text{if we decide salmon}\end{cases}</script><p>在第二部分贝叶斯决策规则的最后一条写道：最好的决策是最小化错误率，也就是说，<strong>我们要最小化平均错误率。</strong></p><p>因为平均错误率由：</p><script type="math/tex; mode=display">P(error)=\int_{-\infty}^{+\infty}P(error,x)dx=\int_{-\infty}^{+\infty}P(error|x)p(x)dx</script><p>得出，所以我们只要对每个x，尽可能最小化$P(error|x)$，这样积分就会变小。因此，错误率公式可以写作$P(error | x)=min[P(\omega_1|x),P(\omega_2|x)]$</p><h1 id="贝叶斯理论——连续特征"><a href="#贝叶斯理论——连续特征" class="headerlink" title="贝叶斯理论——连续特征"></a>贝叶斯理论——连续特征</h1><p>上面到情况是假设每个错误的权重（这个权重是指，比如银行猜测一个人是否是歹徒，有两种错误，一种是其实是歹徒但是猜成了不是，另一种是其实不是歹徒但是猜成了是，这种情况下，我们宁可第二种错误发生，也不希望第一种错误发生，所以这就产生了每个错误的权重）一样，现在我们从四个方面对贝叶斯决策理论进行泛化：</p><ul><li>泛化到多个特征</li><li>泛化到多个类</li><li>泛化到除了对类进行决策外还能有其他行为，比如：拒绝决策</li><li>将错误率泛化到损失函数（代价函数）</li></ul><p>前三个泛化不难理解，第四个泛化适用于第二部分的最后一条提到的“每个错误的权重（代价）不一样”的情况。</p><p>我们用$\omega_{1},\omega_{2},…,\omega_{c}$表示$c$个类，用$\alpha_{1}, \alpha_{2}, …,\alpha_{a}$表示可以采取的a个动作(其实这个动作就可以理解为猜测，比如$\alpha_1$代表猜这个东西的类为$\omega_1$)。当实际为$\omega_{j}$，行动为$\alpha_{i}$时(也就是猜成$\omega_i$)，损失函数为$\lambda\left(\alpha_{i}|\omega_{j}\right)$。特征向量$\overrightarrow x$表示一个$d$维的随机向量，$p(\overrightarrow x|\omega_{j})$表示$x$的条件密度函数。$P(\omega_{j})$表示$\omega_{j}$的先验概率。则后验概率可以用贝叶斯公式计算出来：</p><script type="math/tex; mode=display">P\left(\omega_{j} | \overrightarrow x \right) = \frac{p\left(\overrightarrow x|\omega_{j}\right)P\left(\omega_{j}\right)}{p\left(\overrightarrow x\right)}</script><p>其中，$p(\overrightarrow x)$为：</p><script type="math/tex; mode=display">p(\overrightarrow x)=\sum^c_{j=1}p(\overrightarrow x|\omega_j)P(\omega_j)</script><p>假设我们针对一个具体的特征向量$\overrightarrow x$,采取行动$\alpha_i$(即猜类别为$\omega_i$),而实际类别是$\omega_j$,则损失函数为$\lambda(\alpha_i|\omega_j)$。因为后验概率为$P(\omega_j|\overrightarrow x)$, 所以采取行动$\alpha_i$的损失为</p><script type="math/tex; mode=display">R(\alpha_i|\overrightarrow x)=\sum^c_{j=1}\lambda(\alpha_i|\omega_j)P(\omega_j|\overrightarrow x)</script><p>在决策理论的术语中，可预期的损失叫做风险$risk$，$R(\alpha_i|\overrightarrow x)$叫做条件风险$conditional\ risk$。</p><p>我们现在根据整体的risk来优化贝叶斯决策规则，也就是说，<strong>在所有错误的权重不相等的情况下， 我们采取使整体风险最小的行动。</strong></p><p>为了得到好的结果，我们需要最小化整体风险，即对每个x都采取条件风险最小的$action$，用$\alpha(x)$表示，最终使整体风险：</p><script type="math/tex; mode=display">R=\int R(\alpha(\overrightarrow x)|\overrightarrow x)p(\overrightarrow x)\ d\overrightarrow x</script><p>最小化。</p><p>可想而知，如果对每个$\overrightarrow x$，$R(\alpha(\overrightarrow x)|\overrightarrow x)$都尽可能小，那么整体风险就会最小，也就是说，计算：</p><script type="math/tex; mode=display">R(\alpha(\overrightarrow x)|\overrightarrow x)=\sum_{j=1}^c\lambda(\alpha_i|\omega_i)P(\omega_j|\overrightarrow x), \forall i=1,2, ..., a</script><p>采取使其最小的行动$\alpha_i$。</p><p>最小化的整体风险被称为贝叶斯风险$Bayes\ risk$，用$R^* $表示。<br></p><h2 id="二分类问题"><a href="#二分类问题" class="headerlink" title="二分类问题"></a>二分类问题</h2><p>我们现在考虑一个二分类问题。我们用$\alpha_1$表示猜测为$\omega_1$，$\alpha_2$表示猜测为$\omega_2$。为了符号简洁，我们用$\lambda_{ij}=\lambda(\alpha_i|\omega_j)$表示实际是$\omega_j$却猜成$\alpha_i$的损失。根据条件风险的公式我们可以得到</p><script type="math/tex; mode=display">R(\alpha_1|\overrightarrow x) = \lambda_{11}P(\omega_1|\overrightarrow x) + \lambda_{12}P(\omega_2|\overrightarrow x)</script><p>以及</p><script type="math/tex; mode=display">R(\alpha_2|\overrightarrow x) = \lambda_{21}P(\omega_1|\overrightarrow x) + \lambda_{22}P(\omega_2|\overrightarrow x)</script><p>我们的规则是，采取风险最小的行动，即猜做$\omega_1$如果$R(\alpha_1|\overrightarrow x) &lt; R(\alpha_2|\overrightarrow x)$，否则$\omega_2$。</p><p>有很多方式可以表示这一规则，各有优势。</p><ol><li><p>猜做$\omega_1$如果：</p><script type="math/tex; mode=display">(\lambda_{21}-\lambda_{11})P(\omega_1|\overrightarrow x)>(\lambda_{12}-\lambda_{22})P(\omega_2|\overrightarrow x)</script><p>一般来讲，猜错的损失要大于猜对的损失，所以$(\lambda_{21}-\lambda_{11})$和$(\lambda_{12}-\lambda_{22})$都大于0，因此我们的决定就更依赖于后验概率，根据贝叶斯公式，可以得到</p><script type="math/tex; mode=display">(\lambda_{21}-\lambda_{11})p(\overrightarrow x|\omega_1)P(\omega_1)>(\lambda_{12}-\lambda_{22})p(\overrightarrow x|\omega_2)P(\omega_2)</script><p>变成这样就和上面提到的贝叶斯决策理论（简单版）一样了。</p></li><li><p>猜做$\omega_1$如果：</p><script type="math/tex; mode=display">\dfrac{p(\overrightarrow x|\omega_1)}{p(\overrightarrow x|\omega_2)}>\dfrac{\lambda_{12}-\lambda_{22}}{\lambda_{21}-\lambda_{11}}\dfrac{P(\omega_2)}{P(\omega_1)}</script><p>此公式是另一个变换方式。这个公式侧重x的概率密度，我们可以把$p(\overrightarrow x|\omega_j)$想象为$\omega_j$的函数（如，似然函数likelihood function），然后产生了似然比（likelihood radio）$p(\overrightarrow x|\omega_1)/p(\overrightarrow x|\omega_2)$。因此贝叶斯规则可以被解释成<strong>如果似然比超过不依赖于x的某阈值，则猜做$\omega_1$。</strong></p></li></ol><h1 id="最小错误率的分类"><a href="#最小错误率的分类" class="headerlink" title="最小错误率的分类"></a>最小错误率的分类</h1><p>如果想避免错误，自然会想到寻求一种能够最小化错误率的决策规则，比如所谓的symmetrical损失函数或者zero-one损失函数。</p><script type="math/tex; mode=display">\lambda(\alpha_i|\omega_j)=\begin{cases}0& i=j\\\\1& i\neq j\end{cases}\ \ \ \  i,j=1,\dots, c.</script><p>这个损失函数如果猜对了就没有损失，如果猜错了就会有一个单位的损失，因此所有的错误代价等价。这个损失函数的风险可以被视作平均错误率，因此条件风险是</p><script type="math/tex; mode=display">\begin{aligned}R(\alpha_i|\overrightarrow x)&=\sum_{j=1}^c\lambda(\alpha_i|\omega_j)P(\omega_j|\overrightarrow x) \\& = \sum_{j \neq i}P(\omega_j|\overrightarrow x) \\& = 1 - P(\omega_i|\overrightarrow x)\end{aligned}</script><p>而$P(\omega_i|\overrightarrow x)$是猜测$\alpha_i$正确的条件概率，贝叶斯决策规则是最小化风险，因此我们需要选择最大化后验概率$P(\omega_i|\overrightarrow x)$的$i$。就得到了规则</p><script type="math/tex; mode=display">Decide\ \omega_i\ if\ P(\omega_i|\overrightarrow x)>P(\omega_j|\overrightarrow x) \ \ for\ all\ j\neq i</script><p>其实与上文理解代价的部分错误规则相同。下图是相同情况下的似然比图。<br><img src="/imgs/MIT-PatternRecognish3-1.png" alt="likelihood ratio in the same case"><br>通常来讲，似然比范围是0到无穷，阈值$\theta_a$表示先验概率相同但是损失函数是zero-one损失函数。如果我们对猜错$\omega_1$对的处罚跟大些，阈值就要上移至$\theta_b$.</p><h2 id="最大最小判定"><a href="#最大最小判定" class="headerlink" title="最大最小判定"></a>最大最小判定</h2><p>有时候不知道先验概率或者先验概率变化很大，此时我们依然希望分类起器有很好的表现，这时候就需要设计一个分类器，让不管先验概率是什么值都能使风险最小，也就是说，<strong>最小化最大的整体风险</strong>。</p><p>我们用$R_1$代表分类起器认为是$\omega_1$类的特征空间，$R_2$同理。则整体的条件风险为</p><script type="math/tex; mode=display">\begin{aligned}R & = \int_{R_1}[\lambda_{11}P(\omega_1)p(\overrightarrow x|\omega_1)+\lambda_{12}P(\omega_2)p(\overrightarrow x|\omega_2)]\ d\overrightarrow x \\& + \int_{R_2}[\lambda_{21}P(\omega_1)p(\overrightarrow x|\omega_1)+\lambda_{22}P(\omega_{2})p(\overrightarrow x|\omega_2)]\ d\overrightarrow x\end{aligned}</script><h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><ul><li>MIT课程指定的Reading：Duda, Richard O., Peter E. Hart, and David G. Stork. <em>Pattern Classification</em>. New York, NY: John Wiley &amp; Sons, 2000. ISBN: 9780471056690.</li><li><a href="https://www.youtube.com/watch?v=mfePdDh9t6Q&amp;list=PLbMVogVj5nJQJMLb2CYw9rry0d5s0TQRp" target="_blank" rel="noopener">印度的讲的贼好的MOOC第三节</a></li></ul>]]></content>
      
      <categories>
          
          <category> MIT </category>
          
          <category> Pattern Recognition </category>
          
      </categories>
      
      
        <tags>
            
            <tag> MIT课程 </tag>
            
            <tag> 模式识别 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>模式识别与分析——贝叶斯决策理论（数学部分）</title>
      <link href="/2018/06/07/MIT-PatternRecognition2/"/>
      <url>/2018/06/07/MIT-PatternRecognition2/</url>
      <content type="html"><![CDATA[<h1 id="贝叶斯决策理论相关数学知识"><a href="#贝叶斯决策理论相关数学知识" class="headerlink" title="贝叶斯决策理论相关数学知识"></a>贝叶斯决策理论相关数学知识</h1><p>主要是概率论，如果你这部分基础牢固，可以跳过，直接看理论部分。<br><a id="more"></a></p><h2 id="概率质量函数"><a href="#概率质量函数" class="headerlink" title="概率质量函数"></a>概率质量函数</h2><p>概率质量函数（Probability Mass Function）是针对离散值而言的，通常用大写字母P表示。假设某个事件$\omega_{1}$发生的概率为$P(\omega_{1})$,某个事件$\omega_{2}$发生的概率为$P(\omega_{2})$,两事件相互独立，则$P(\omega_{1})+P(\omega_{2})=1$。</p><h2 id="概率密度函数"><a href="#概率密度函数" class="headerlink" title="概率密度函数"></a>概率密度函数</h2><p>概率密度函数（Probability Desity Function）是针对连续值而言的，通常用小写字母$p$表示。<strong>概率密度函数的在正无穷到负无穷上到积分为1</strong>，在某一个区间中的概率用在该区间中的积分来表示。</p><p>用数学语言描述就是：</p><p>$<br>(1)p(\overrightarrow {x}) \geq 0, \forall\ \overrightarrow {x}\in R^n<br>$<br>$<br>(2)\int p(\overrightarrow {x})\ d\ \overrightarrow x=1<br>$</p><p><strong>NOTE</strong>：</p><ul><li>$\overrightarrow x$是一个列向量</li></ul><p>任何满足以上两个条件的函数都叫在n为<a href="https://en.wikipedia.org/wiki/Euclidean_space" target="_blank" rel="noopener">欧几里得空间</a>（Euclidean Space）上的概率密度函数。</p><p>比如：</p><h3 id="高斯密度函数"><a href="#高斯密度函数" class="headerlink" title="高斯密度函数"></a>高斯密度函数</h3><p>高斯密度函数（Gaussian Density Function or Density Function for Gaussian Distribution）的定义为：</p><script type="math/tex; mode=display">p(\overrightarrow x)=\dfrac{1}{(\sqrt{2\pi})^n|\Sigma|^{1/2}}exp\left\{-\dfrac{1}{2}(\overrightarrow x-\overrightarrow \mu)^T\Sigma^{-1}(\overrightarrow x - \overrightarrow \mu)\right\}</script><p><strong>NOTE：</strong></p><ul><li><p>可能发现上面那个公式和平时见的公式长得不太一样，其实它是从线性代数的角度写的。</p></li><li><p>公式中的$|\Sigma|$代表Determinant of sigma,  也就是$\Sigma$的行列式，将nxn的矩阵映射成一个标量（既然提到了行列式并且我也有些遗忘，所以一会儿在文末附录里整理一下它的概念）。$\Sigma$是什么呢？它叫Variance-Covariance Matrix， 也叫Dispersion Matrix，是一个nxn的矩阵，它的逆$\Sigma^{-1}$也是一个nxn的矩阵。（这里协方差矩阵和矩阵的逆还有矩阵的转置，也要在附录里温习）ok，回归正题，这个determinant of sigma可能是0也可能是负数，但是如果是负数，1/2次方就会很难计算，因为它会得到一个非常复杂的数， 而我们的概率密度函数的第一个条件就是$p(\overrightarrow x)\geq0$，<strong>所以determinant ofsigma必须大于0</strong>， 因为即使是等于0，1/0也无法计算。</p></li><li>$exp$代表e的某次方。</li><li>$\overrightarrow x$：一个$n$维的向量</li><li>$\overrightarrow \mu$：均值向量，代表分布的均值，也是一个$n$维的向量（mean vector同样在附录里温习）</li><li><p>因为$\overrightarrow x$和$\overrightarrow \mu$都是n维的列向量，所以$(\overrightarrow x-\overrightarrow \mu)$也是一个n维的列向量，即nx1的矩阵，所以$(\overrightarrow x-\overrightarrow \mu)^T$是一个n维的行向量， 即1xn的矩阵</p></li><li><p>所以$(\overrightarrow x-\overrightarrow \mu)^T\Sigma^{-1}(\overrightarrow x - \overrightarrow \mu)$是一个标量，所以这一项是e的任何大于等于0的次方。</p></li></ul><p>看完这里，请<strong>跳到附录</strong>，补充<strong>Variance-Covariance Matrix和Positive-Definite Matrix</strong> 的概念，至于行列式（Determinant）和矩阵的逆以及矩阵的转置，看不看都行。</p><h2 id="先验概率"><a href="#先验概率" class="headerlink" title="先验概率"></a>先验概率</h2><p>先验概率（Prior Probability）是指根据已有情况提前知道的概率，比如已知有一箱红黑混合的小球，其中红色小球共有100颗，黑色小球共有200颗，则红色小球的先验概率为$P(red) = 1/3$, 黑色小球的先验概率为$P(black) = 2/3$。</p><h2 id="条件概率"><a href="#条件概率" class="headerlink" title="条件概率"></a>条件概率</h2><p>假设将上述红黑混合的小球们放在两个箱子中，即A箱放20个红色小球，100个黑色小球，B箱放80个红色小球，100个黑色小球，则从A中取到红色小球的概率是多少？这就是条件概率。</p><script type="math/tex; mode=display">P(red|A) = P(red \& A) / P(A) = (20 / 300) / (120 / 300) = 1/6</script><p>那么，红色里面来自A的概率是多少呢？</p><script type="math/tex; mode=display">P(A|red) = P(A \& red) / P(red) = (20 / 300) / (100 / 300) =  1 / 5</script><h1 id="附录"><a href="#附录" class="headerlink" title="附录"></a>附录</h1><h2 id="Variance-Covariance-Matrix"><a href="#Variance-Covariance-Matrix" class="headerlink" title="Variance-Covariance Matrix"></a>Variance-Covariance Matrix</h2><p>首先需要知道Variance和Covariance的定义。</p><h3 id="Variance"><a href="#Variance" class="headerlink" title="Variance"></a>Variance</h3><p>假设有n个observations：<script type="math/tex">x_1, x_2, x_3, ..., x_n \in R</script></p><p>它们的平均数$\bar x$等于:</p><script type="math/tex; mode=display">\bar x=\dfrac {1}{n}\sum_{i=1}^{n}x_i</script><p>它们的方差Variance等于</p><script type="math/tex; mode=display">Variance=\dfrac {1}{n}\sum_{i=1}^{n}(x_i-\bar x)^2</script><blockquote><p>一些书也会写为：$<br>Variance=\dfrac {1}{n-1}\sum_{i=1}^{n}(x_i-\bar x)^2$，这实际上是unbias estimate for Variance of the population,与1/n在value上有些差别，这是统计学中比较复杂的一个概念。（老师没有做详细介绍，说可以课后去查，而我也不打算深入此概念，所以和老师一样，variance就follow第一种写法。）</p></blockquote><h3 id="Covariance"><a href="#Covariance" class="headerlink" title="Covariance"></a>Covariance</h3><p>为了阐释协方差Covariance，我们需要两个变量(x, y)，假设x是身高，单位是cm，y是体重，单位是kg。<br>假设有n个observations：</p><script type="math/tex; mode=display">(x_1,y_1),(x_2, y_2), ..., (x_n, y_n)</script><p>你要做的是plot these points，这里我给出三个这样的plots，灰色区域是一些列点：<br><img src="/imgs/MIT-PatternRecognition2-1.JPG" alt="plot the points"><br>图（1）中，x增长，y随x也增长，所以我们用一些大于0的数（quantity）来代表这个关系；<br>图（2）中，x增长，y随x减小，我们用一些小于0的数（quantity）来代表这个关系；<br>图（3）中，x增长，y在某一个范围内波动，所以我们用一些非常接近0的数（quantity）来代表这个关系。</p><p>那么这个数（quantity）到底是什么呢？</p><p>对于所有的x和y，我们找到它们的均值，然后将其作为新坐标轴的原点：<br><img src="/imgs/MIT-PatternRecognition2-2.JPG" alt="new axis"><br>那么所有点的x，y值都会变化，把这些新的值乘起来求均值，会得到什么呢？</p><p>比如图（1），新坐标系第一象限的x，y都大于0，乘积也会大于0，第三象限x，y都小于0，乘积也会大于0，第二和第四象限乘积会小于0，但是一三象限的点数量明显大于二四象限的点，所以我们计算</p><script type="math/tex; mode=display">\dfrac{1}{n}\sum_{i=1}^n(x_i - \bar x)(y_i - \bar y)</script><p>会得到一个大于0的值。</p><p>同理图（2）会得到一个小于0的值，图（3）会得到一个约等于0的值。</p><p>这就是x和y的协方差Covariance</p><script type="math/tex; mode=display">Cov(x,y)=\dfrac{1}{n}\sum_{i=1}^n(x_i - \bar x)(y_i - \bar y)</script><p>可以看出，$Cov(x,x)$就是variance。</p><h3 id="Variance-Covariance-Matrix-1"><a href="#Variance-Covariance-Matrix-1" class="headerlink" title="Variance-Covariance Matrix"></a>Variance-Covariance Matrix</h3><p>在模式识别中，我们把这一系列变量称作features，如果两两组合，会得到多少对呢？$n^2$对。<br>如果n个features是</p><script type="math/tex; mode=display">x_1, x_2, x_3, \dots,x_n</script><p>则这n个features的Variance-Covariance matrix为：</p><script type="math/tex; mode=display">\Sigma=\begin{bmatrix}{Cov(x_1,x_1)}&{Cov(x_1, x_2)}&{\cdots}&{Cov(x_1, x_n)}\\{Cov(x_2,x_1)}&{Cov(x_2,x_2)}&{\cdots}&{Cov(x_2,x_n)}\\{\vdots}&{\vdots}&{\ddots}&{\vdots}\\{Cov(x_n,x_1)}&{Cov(x_n,x_2)}&{\cdots}&{Cov(x_n,x_n)}\\\end{bmatrix}</script><p>这是一个对称矩阵symmetric matrix，也是一个正定矩阵<a href="https://en.wikipedia.org/wiki/Positive-definite_matrix" target="_blank" rel="noopener">Positive-definite matrix</a>，什么是正定矩阵呢，往下看。</p><h2 id="Positive-definit-matrix"><a href="#Positive-definit-matrix" class="headerlink" title="Positive-definit matrix"></a>Positive-definit matrix</h2><p>大家应该都知道“欧几里得”距离是什么吧，假设我们有一个列向量$\overrightarrow x=[x_1, x_2, \dots,x_n]$和一个列向量$\overrightarrow y=[y_1, y_2, \dots, y_3]$,则x和y的欧几里得距离为$d(\overrightarrow x, \overrightarrow y)=\sqrt{\sum_{i=1}^n(x_i-y_i)^2}$。</p><p>现在假设x代表第一个人的feature，y代表第二个人的feature，每个列向量只有两列，分别代表身高和体重。</p><p>x的身高和体重分别为160cm和70kg，y的身高和体重分别为158cm和73kg，现在想衡量x和y的距离，如果用上面的欧式距离，就会有些问题，为什么这么说呢？</p><script type="math/tex; mode=display">d(\overrightarrow x, \overrightarrow y)=\sqrt{(160-158)^2+(70-73)^2}=\sqrt{13}</script><p>如果同样是这两个人，把身高的单位换成mm，同样的方式计算x和有的距离：</p><script type="math/tex; mode=display">d(\overrightarrow x, \overrightarrow y)=\sqrt{(1600-1580)^2+(70-73)^2}=\sqrt{409}</script><p>这是我们不期望得到的结果，相同的两个人，衡量他们的距离，应该无论如何都始终一样，而非仅仅换了单位就出现不同的结果。</p><p>所以欧式距离往往是not useful的。</p><p>现在让我们移除公式的根号：</p><script type="math/tex; mode=display">d^2(\overrightarrow x, \overrightarrow y)=(x_1-y_1, x_2-y_2)\left(\begin{array}{cccc}    1 &    0 \\    0 &    1\\\end{array}\right) \left(\begin{array}{cccc}    x_1-y_1 \\    x_2-y_2 \\\end{array}\right)</script><p>这种写法与$d(\overrightarrow x, \overrightarrow y)=\sqrt{\sum_{i=1}^2(x_i-y_i)^2}$是等价的，一个矩阵与单位矩阵（identity matrix）相乘是不变的。</p><p>现在我们对中间对单位矩阵做一些泛化，把它改成$\left(\begin{array}{cccc}<br>    w_1 &amp;    0 \\<br>    0 &amp;    w_2\\<br>\end{array}\right)$，则相应的距离公式变为$d(\overrightarrow x, \overrightarrow y)=\sqrt{\sum_{i=1}^2w_i(x_i-y_i)^2}$，这里的$w_i$取决于单位，这样就能解决我们的问题：当单位发生改变时，相同的两个人距离不发生改变。这种定义下，如果$w_1$和$w_2$严格大于等于0，那么最后的<strong>距离就是大于或者等于0的</strong></p><p>你还可以继续泛化，把单位矩阵改成$\left(\begin{array}{cccc}<br>    w_{11} &amp;    w_{12} \\<br>    w_{21} &amp;    w_{22} \\<br>\end{array}\right)$，在这种定义下，w的值该取多少距离才会大于0呢？<br>满足这样的w有很多，比如随便举个例子：</p><script type="math/tex; mode=display">(x_1-y_1, x_2-y_2)\left(\begin{array}{cccc}    2 &    -1 \\    -1 &    2 \\\end{array}\right)\left(\begin{array}{cccc}    x_1-y_1 \\    x_2-y_2\\\end{array}\right) > 0, \ if(x_1 \neq y_1)\ or\ (x_2 \neq y_2)</script><p>这样我们的Positive-definite matrix就有了定义：</p><p>$A_{n*n}$ 可以被称作是positive-definite的，当满足：</p><script type="math/tex; mode=display">a^TAa > 0, \ \forall a \neq \left(\begin{array}{cccc}    0 \\    0 \\    0 \\    . \\    . \\    . \\    0\end{array}\right)_{n*1}</script><p>如果</p><script type="math/tex; mode=display">a^TAa \geq 0, \ \forall a.</script><p>那么$A_{n*n}$可以被称作是non-negative definite.</p><p>在线性代数中，很容易找到positive-definite matrix的定义，那么我们为什么需要一个这样的矩阵呢？从上面那个“距离”的角度来说，我们需要这样的矩阵是因为我们要确保距离是大于等于0的，如果x和y是完全一样的，我们希望其距离为0，否则我们希望一个大于0的数来表示不同程度，因此我们把距离公式写成$a^TAa$的矩阵形式。</p><p>Variance-Covariance matrix  可以被证明是NON-NEGATIVE DEFINITE的，实际上通常是POSITIVE-DEFINITE的，在正态分布（高斯密度函数）下，我们认为Variance-Covariance matrix 是POSITIVE-DEFINITE的，这就是我们为什么会在高斯密度函数的分母上把它写成$|\Sigma|^{1/2}$的原因。如果Variance-Covariance matrix是non-negative definite的，就会有一些properties，比如矩阵的行列式是它的特征值（the determinant of matrix is product of its eigenvalues），所有的特征值都是大于等于0的，如果variance-covariance matrix是positive-definite的，那么所有eigenvalues都是严格大于0的，所以可以把它做分母。</p><h2 id="Determinant"><a href="#Determinant" class="headerlink" title="Determinant"></a>Determinant</h2><p>在线性代数里，Determinant是一个可以从方形矩阵中计算出来的值。矩阵的Determinant记做$det(A)\ or \ detA \ or \ |A|$。在几何学里，它被视作描述矩阵线性变换的scaling factor。</p><p>2x2的矩阵行列式计算方法为：</p><script type="math/tex; mode=display">|A|=\left|\begin{array}{cccc}    a & b  \\    c & d \\\end{array}\right|=ad - bc</script><p>更高阶的计算方法到参考文献的链接查看吧，mathjax不好写了。</p><h2 id="Matrix-inverse"><a href="#Matrix-inverse" class="headerlink" title="Matrix inverse"></a>Matrix inverse</h2><p>在线性代数中，如果一个nxn的方阵A存在一个nxn的方阵B使其满足</p><script type="math/tex; mode=display">AB=BA=I_n</script><p>则称A为可逆矩阵，B是A的逆。$I_n$是nxn的Identity Matrix，也被含糊地称为Unit Matrix，单位矩阵，对角线是1，其余是0。</p><h2 id="Transpose-of-Matrix"><a href="#Transpose-of-Matrix" class="headerlink" title="Transpose of Matrix"></a>Transpose of Matrix</h2><p>在线性代数里，矩阵的转置就是行列元素的索引对调，记做$A^T$:</p><p>$$<br>[A^T]_{ij}=[A]_{ji}</p><p>$$</p><h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><ul><li><a href="https://www.youtube.com/watch?v=mfePdDh9t6Q&amp;list=PLbMVogVj5nJQJMLb2CYw9rry0d5s0TQRp" target="_blank" rel="noopener">印度的讲的贼好的MOOC</a></li><li><a href="https://en.wikipedia.org/wiki/Determinant" target="_blank" rel="noopener">矩阵行列式（Determinant）</a></li><li><a href="https://en.wikipedia.org/wiki/Invertible_matrix" target="_blank" rel="noopener">矩阵的逆（Inverse）</a></li><li><a href="https://en.wikipedia.org/wiki/Transpose" target="_blank" rel="noopener">矩阵的转置</a></li></ul>]]></content>
      
      <categories>
          
          <category> MIT </category>
          
          <category> Pattern Recognition </category>
          
      </categories>
      
      
        <tags>
            
            <tag> MIT课程 </tag>
            
            <tag> 模式识别 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>模式识别与分析——简介及大纲</title>
      <link href="/2018/06/06/MIT-PatternRecognition/"/>
      <url>/2018/06/06/MIT-PatternRecognition/</url>
      <content type="html"><![CDATA[<p>越往后研究，越发现数学是瓶颈，很多模型只停留在会用的阶段，稍遇到棘手的问题就发现自己现有的学识是多么浅薄，所以决定放下手头亟待解决的问题，静下来磨刀练剑。找到2006年秋季<a href="https://ocw.mit.edu/courses/media-arts-and-sciences/mas-622j-pattern-recognition-and-analysis-fall-2006/index.htm" target="_blank" rel="noopener">MIT的一门课程</a>，英文名叫<em>Pattern Recognition and Analysis</em>，课程只给了Readings，是<em>Pattern Classification</em>，作者是Duda，结合一个印度的<a href="https://www.youtube.com/watch?v=mfePdDh9t6Q&amp;list=PLbMVogVj5nJQJMLb2CYw9rry0d5s0TQRp" target="_blank" rel="noopener">讲的很好的MOOC</a>，重新学习一下模式识别，在这里记下学习笔记（注意，这些文字只是我一段时光的记录，<strong>不是教程！不是教程！不是教程！</strong>）。当然，如果碰巧你也在学习这部分知识，可以一起探讨前行，感谢指出问题与不足。<br><a id="more"></a></p><p>每个知识点的学习思路包含以下三个部分：</p><ul><li>数学部分（与知识点紧密相关的数学知识）</li><li>理论部分（知识点的详细介绍）</li><li>实战部分（知识点对应的代码实现，如果我找到合适的项目应用，也会一并写出来，开源）</li></ul><h1 id="MIT的大纲"><a href="#MIT的大纲" class="headerlink" title="MIT的大纲"></a>MIT的大纲</h1><p>不出意外的话我的学习线路也会跟着这个大纲。</p><p>这门课涉及了表征和识别数字数据中的模式及特征的基础知识，讨论了用户建模(user modeling), 影响识别(affect recognition), 语音识别(speech recognition), 计算机视觉(computer vision), 生理分析(physiological analysis)等应用等基本工具和理论，覆盖了决策理论(decision theory)， 统计分类(statistical classification)，最大似然(maximum likelihood)和贝叶斯估计(Bayesian estimation)， 非参数方法(nonparametic methods)， 无监督学习(unsupervised learning)和聚类(clustering)，还包含了一些来自active reaserch等机器和人类学习相关话题。</p><ul><li>介绍模式识别(Pattern Recognition)，特征检测(Feature Detection)，分类(classification)</li><li>综述概率论(Probability Theory)，条件概率(Conditional Probability)和贝叶斯规则(Bayes Rule)</li><li>随机向量(Random Vectors)，期望(Expectation)，相关(Correlation)，协方差(Covariance)</li><li>综述线性代数(Linear Algebra)，线性变换(Linear Transformations)</li><li>决策理论(Decision Theory)， ROC曲线(ROC Curves), 似然比检验(likelihood Ratio Test)</li><li>线性和二次判别式(Linear and Quadratic Discriminants)，Fisher判别式(Fisher Discriminate)</li><li>充分统计量(Sufficient Statistics)，应对缺失或噪声特征</li><li>基于模版等识别(Template-based Recognition)，特征提取(Feature Extraction)</li><li>特征向量(Eigenvector)与多重线性分析(Multilinear Analysis)</li><li>训练方法，最大似然和贝叶斯参数估计</li><li>线性判别/感知器学习，梯度下降优化</li><li>支持向量机</li><li>K-近邻分类</li><li>非参数分类，密度估计，Parzen估计</li><li>无监督学习，聚类，矢量量化(Vector Quantization)， K-Means</li><li>混合建模， 期望最大化</li><li>隐马尔可夫模型， Viterbi算法， Baum-Welch算法</li><li>线性动态系统，卡尔曼滤波</li><li>贝叶斯网络</li><li>决策树， 多层感知器</li><li>人机交互中的强化学习</li><li>遗传算法(Genetic Algorithms)</li><li>多分类器“Committee Machines”的组合</li></ul>]]></content>
      
      <categories>
          
          <category> MIT </category>
          
          <category> Pattern Recognition </category>
          
      </categories>
      
      
        <tags>
            
            <tag> MIT课程 </tag>
            
            <tag> 模式识别 </tag>
            
        </tags>
      
    </entry>
    
  
  
    
    <entry>
      <title>about</title>
      <link href="/about/index.html"/>
      <url>/about/index.html</url>
      <content type="html"><![CDATA[<p></p>]]></content>
    </entry>
    
    <entry>
      <title>categories</title>
      <link href="/categories/index.html"/>
      <url>/categories/index.html</url>
      <content type="html"><![CDATA[]]></content>
    </entry>
    
    <entry>
      <title>tags</title>
      <link href="/tags/index.html"/>
      <url>/tags/index.html</url>
      <content type="html"><![CDATA[]]></content>
    </entry>
    
  
</search>
