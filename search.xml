<?xml version="1.0" encoding="utf-8"?>
<search> 
  
    
    <entry>
      <title>模式识别与分析——贝叶斯决策理论（理论部分）</title>
      <link href="/2018/06/07/MIT-PatternRecognition2/"/>
      <url>/2018/06/07/MIT-PatternRecognition2/</url>
      <content type="html"><![CDATA[<h2 id="先修知识"><a href="#先修知识" class="headerlink" title="先修知识"></a>先修知识</h2><h4 id="Probability-Mass-Function"><a href="#Probability-Mass-Function" class="headerlink" title="Probability Mass Function"></a>Probability Mass Function</h4><p>概率质量函数是针对离散值而言的，通常用大写字母P表示。假设某个事件$\omega_{1}$发生的概率为$P(\omega_{1})$,某个事件$\omega_{2}$发生的概率为$P(\omega_{2})$,两事件相互独立，则$P(\omega_{1})+P(\omega_{2})=1$。</p><h4 id="Probability-Desity-Function"><a href="#Probability-Desity-Function" class="headerlink" title="Probability Desity  Function"></a>Probability Desity  Function</h4><p>概率密度函数是针对连续值而言的，通常用小写字母$p$表示。<strong>概率密度函数的在正无穷到负无穷上到积分为1</strong>，在某一个区间中的概率用在该区间中的积分来表示。<br><a id="more"></a></p><h4 id="Prior-Probability"><a href="#Prior-Probability" class="headerlink" title="Prior Probability"></a>Prior Probability</h4><p>先验概率是指根据已有情况提前知道的概率，比如已知有一箱红黑混合的小球，其中红色小球共有100颗，黑色小球共有200颗，则红色小球的先验概率为$P(red) = 1/3$, 黑色小球的鲜艳概率为$P(black) = 2/3$。</p><h4 id="条件概率"><a href="#条件概率" class="headerlink" title="条件概率"></a>条件概率</h4><p>假设将上述红黑混合的小球们放在两个箱子中，即A箱放20个红色小球，100个黑色小球，B箱放80个红色小球，100个黑色小球，则从A中取到红色小球的概率是多少？这就是条件概率。</p><p>$$P(red|A) = P(red \&amp; A) / P(A) = (20 / 300) / (120 / 300) = 1/6$$</p><p>那么，红色里面来自A的概率是多少呢？</p><p>$$P(A|red) = P(A \&amp; red) / P(red) = (20 / 300) / (100 / 300) =  1 / 5$$</p><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>贝叶斯决策理论（Bayesian decision theory）是一种根据概率进行决策的理论，在模式识别中，将分类当作决策进行预测。</p><p><strong>贝叶斯公式</strong></p><p>$$P\left(\omega_{j} | x \right) = \frac{p\left(x|\omega_{j}\right)P\left(\omega_{j}\right)}{p\left(x\right)}$$</p><h2 id="结合实例理解"><a href="#结合实例理解" class="headerlink" title="结合实例理解"></a>结合实例理解</h2><h4 id="初级"><a href="#初级" class="headerlink" title="初级"></a>初级</h4><p>例如要将两种鱼sea bass和salmon进行分类，假设只有这两种鱼，随机取一条鱼，让我们猜这条鱼是什么鱼，我们第一想到的就是哪种鱼占总数数量多就猜哪种鱼，也就是根据先验概率进行猜测，即如果$P(sea \ bass)&gt;P(salmon)$，那么我们猜是sea bass，这样猜对的概率更大些，反之亦然。</p><p>但是上述猜测方式只适合猜一次，试想如果猜测很多次，每次我们都猜是sea bass或者salmon，得到的结果一定不尽人意，毕竟我们知道，两种鱼都有可能出现，随着实验次数的增加，我们猜错的可能性是$P(sea\ bass)$和$P(salmon)$中更小的一个。</p><p>通常情况下，我们做决策的依据可能不止一个，即我们会有更多特征来辅助决策。例如我们还知道鱼的光泽度$x$，假设$x$是一个随机连续变量，那么我们的猜测策略就要从刚才的纯根据先验概率升级为根据条件概率进一步猜测。想象一个场景，随机摸一条鱼，然后有人跟我们说，它的光泽度是5，那么我们会想，光泽度为5时，是sea bass的可能性大呢，还是salmon的可能性大呢？</p><p>那么我们就需要计算$p(sea\ bass|x=5)$和$p(salmon|x=5)$，比较这两个值，取大的做猜测结果。</p><p>想想我们已知的有哪些？首先我们知道，$P(sea\ bass)$和$P(salmon)$, 其次我们知道每种鱼的光泽度分布，即$p(x|sea\ bass)$和$p(x|salmon)$。</p><p>ok，现在根据已知计算$p(sea\ bass|x)$和$p(salmon|x)$。以$p(sea bass|x)$为例：</p><ul><li><p>(1): $p(sea\ bass|x) = p(sea\ bass\ \&amp;\ x) / p(x)$</p></li><li><p>(2): $p(x|sea\ bass) = p(x\ \&amp;\ sea\ bass) / P(sea\ bass)$</p></li><li><p>(3): 由(2)得：$$p(sea\ bass\ \&amp;\ x) =  p(x|sea\ bass) * P(sea\ bass)$$</p></li><li><p>(4): 把(3)带入(1)得：$$p(sea\ bass|x) = p(x|sea\ bass) * P(sea\ bass) / p(x)$$</p></li></ul><p>使用上述方法就可以计算出p(sea bass|x)的概率，p(sea bass|x)同理，如此就可以给出x=5时候的猜测。<br>上述计算公式(4)就是<strong>贝叶斯公式</strong>，通用写法为：<br>$$P\left(\omega_{j} | x \right) = \frac{p\left(x|\omega_{j}\right)P\left(\omega_{j}\right)}{p\left(x\right)}$$</p><p>其中$\omega$代表某个类，$x$代表某个特征。</p><p>如果我们猜是sea bass，可想而知，我们的错误率是$p(salmon|x)$,否则错误率是$p(sea \ bass|x)$。用公式表达为<br>$$<br>P(error | x)=<br>\begin{cases}<br>P(salmon|x), &amp;\text{if we decide sea bass} &amp;\<br>P(sea \ bass|x), &amp;\text{if we decide salmon}<br>\end{cases}<br>$$</p><p>即$P(error | x)=min[P(sea\ bass|x),P(salmon|x)]$</p><h4 id="拓展"><a href="#拓展" class="headerlink" title="拓展"></a>拓展</h4><p>上面是一个简单的例子，在实际情况中，往往不止一个特征，不止一个类，决策（action）也不止是要分到某个类（比如发现两个分类的概率一样时可以拒绝决策），使用错误率计算代价可能太粗略，或许需要每一个action都计算一下损失函数，因此还需要在上面的基础上进行拓展。</p><p>我们用$\omega_{1},\omega_{2},…,\omega_{c}$表示事件的$c$个类别，用$\alpha_{1}, \alpha_{2}, …,\alpha_{a}$表示可以采取的a个决策(action)。当类别为$\omega_{j}$，行动为$\alpha_{i}$时，损失函数为$\lambda\left(\alpha_{i}|\omega_{j}\right)$。特征向量$x$表示一个$d$维的随机向量，$p\left(x|\omega_{j}\right)$表示$x$的条件密度函数。$P(\omega_{j})$表示$\omega_{j}$的先验概率。则后验概率可以用贝叶斯公式计算出来：<br>$$P\left(\omega_{j} | x \right) = \frac{p\left(x|\omega_{j}\right)P\left(\omega_{j}\right)}{p\left(x\right)}$$<br>其中，$p(x)$为：<br>$$<br>p(x)=\sum^c_{j=1}p(x|\omega_j)P(\omega_j)<br>$$</p><p>假设我们针对一个具体的特征$x$,采取行动$\alpha_i$,实际类别是$\omega_j$,则损失函数为$\lambda(\alpha_i|\omega_j)$,后验概率为$P(\omega_j|x)$, 在决策理论的术语中，可预期的损失叫做风险$risk$，$R(\alpha_i|x)$叫做条件风险$conditional\ risk$，则条件风险为<br>$$<br>R(\alpha_i|x)=\sum^c_{j=1}\lambda(\alpha_i|\omega_j)P(\omega_j|x)<br>$$<br>为了得到好的结果，我们需要最小化整体风险，即每次都采取条件风险最小的$action$，最终使整体风险最小化。</p><p>上述说法用数学如何表示呢？“每次采取条件风险最小的$action$”描述了一个决策规则($decision\ rule$),这个规则可以用函数$\alpha(x)$表示，那么特征$x$的条件风险就是$R(\alpha(x)|x)$，特征$x$的概率是$p(x)$，可想而知，整体风险就是<br>$$<br>R=\int R(\alpha(x)|x)p(x)\ dx<br>$$<br>我们想让整体风险最小化，在计算中只要让每个条件风险最小化即可。最小化的整体风险被称为贝叶斯风险$Bayes\ risk$，是能达到的最佳表现。<br></p><p>## 参考文献</p><ul><li>MIT课程指定的Reading：Duda, Richard O., Peter E. Hart, and David G. Stork. <em>Pattern Classification</em>. New York, NY: John Wiley &amp; Sons, 2000. ISBN: 9780471056690.</li></ul>]]></content>
      
      <categories>
          
          <category> MIT </category>
          
          <category> Pattern Recognition </category>
          
      </categories>
      
      
        <tags>
            
            <tag> MIT课程 </tag>
            
            <tag> 模式识别 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>模式识别与分析——简介及大纲</title>
      <link href="/2018/06/06/MIT-PatternRecognition/"/>
      <url>/2018/06/06/MIT-PatternRecognition/</url>
      <content type="html"><![CDATA[<p>这是2006年秋季<a href="https://ocw.mit.edu/courses/media-arts-and-sciences/mas-622j-pattern-recognition-and-analysis-fall-2006/index.htm" target="_blank" rel="noopener">MIT的一门课程</a>，英文名叫<em>Pattern Recognition and Analysis</em>，课程只给了Readings，是<em>Pattern Classification</em>，作者是Duda，在这里记下我的学习笔记。</p><p>这门课涉及了表征和识别数字数据中的模式及特征的基础知识，讨论了用户建模(user modeling), 影响识别(affect recognition), 语音识别(speech recognition), 计算机视觉(computer vision), 生理分析(physiological analysis)等应用等基本工具和理论，覆盖了决策理论(decision theory)， 统计分类(statistical classification)，最大似然(maximum likelihood)和贝叶斯估计(Bayesian estimation)， 非参数方法(nonparametic methods)， 无监督学习(unsupervised learning)和聚类(clustering)，还包含了一些来自active reaserch等机器和人类学习相关话题。<br><a id="more"></a></p><h1 id="大纲"><a href="#大纲" class="headerlink" title="大纲"></a>大纲</h1><ul><li>介绍模式识别(Pattern Recognition)，特征检测(Feature Detection)，分类(classification)</li><li>综述概率论(Probability Theory)，条件概率(Conditional Probability)和贝叶斯规则(Bayes Rule)</li><li>随机向量(Random Vectors)，期望(Expectation)，相关(Correlation)，协方差(Covariance)</li><li>综述线性代数(Linear Algebra)，线性变换(Linear Transformations)</li><li>决策理论(Decision Theory)， ROC曲线(ROC Curves), 似然比检验(likelihood Ratio Test)</li><li>线性和二次判别式(Linear and Quadratic Discriminants)，Fisher判别式(Fisher Discriminate)</li><li>充分统计量(Sufficient Statistics)，应对缺失或噪声特征</li><li>基于模版等识别(Template-based Recognition)，特征提取(Feature Extraction)</li><li>特征向量(Eigenvector)与多重线性分析(Multilinear Analysis)</li><li>训练方法，最大似然和贝叶斯参数估计</li><li>线性判别/感知器学习，梯度下降优化</li><li>支持向量机</li><li>K-近邻分类</li><li>非参数分类，密度估计，Parzen估计</li><li>无监督学习，聚类，矢量量化(Vector Quantization)， K-Means</li><li>混合建模， 期望最大化</li><li>隐马尔可夫模型， Viterbi算法， Baum-Welch算法</li><li>线性动态系统，卡尔曼滤波</li><li>贝叶斯网络</li><li>决策树， 多层感知器</li><li>人机交互中的强化学习</li><li>遗传算法(Genetic Algorithms)</li><li>多分类器“Committee Machines”的组合</li></ul>]]></content>
      
      <categories>
          
          <category> MIT </category>
          
          <category> Pattern Recognition </category>
          
      </categories>
      
      
        <tags>
            
            <tag> MIT课程 </tag>
            
            <tag> 模式识别 </tag>
            
        </tags>
      
    </entry>
    
  
  
    
    <entry>
      <title>about</title>
      <link href="/about/index.html"/>
      <url>/about/index.html</url>
      <content type="html"><![CDATA[]]></content>
    </entry>
    
    <entry>
      <title>categories</title>
      <link href="/categories/index.html"/>
      <url>/categories/index.html</url>
      <content type="html"><![CDATA[]]></content>
    </entry>
    
    <entry>
      <title>tags</title>
      <link href="/tags/index.html"/>
      <url>/tags/index.html</url>
      <content type="html"><![CDATA[]]></content>
    </entry>
    
  
</search>
