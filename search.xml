<?xml version="1.0" encoding="utf-8"?>
<search> 
  
    
    <entry>
      <title>模式识别与分析——贝叶斯决策理论（理论部分）</title>
      <link href="/2018/06/30/MIT-PatternRecognition3/"/>
      <url>/2018/06/30/MIT-PatternRecognition3/</url>
      <content type="html"><![CDATA[<h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><p>贝叶斯决策理论（Bayesian decision theory）是一种根据概率进行决策的理论，在模式识别中，将分类当作决策进行预测。</p><p><strong>贝叶斯公式</strong></p><p>$$P\left(\omega_{j} | x \right) = \frac{p\left(x|\omega_{j}\right)P\left(\omega_{j}\right)}{p\left(x\right)}$$</p><h1 id="结合实例理解"><a href="#结合实例理解" class="headerlink" title="结合实例理解"></a>结合实例理解</h1><h2 id="初级"><a href="#初级" class="headerlink" title="初级"></a>初级</h2><p>例如要将两种鱼sea bass和salmon进行分类，假设只有这两种鱼，随机取一条鱼，让我们猜这条鱼是什么鱼，我们第一想到的就是哪种鱼占总数数量多就猜哪种鱼，也就是根据先验概率进行猜测，即如果$P(sea \ bass)&gt;P(salmon)$，那么我们猜是sea bass，这样猜对的概率更大些，反之亦然。</p><p>但是上述猜测方式只适合猜一次，试想如果猜测很多次，每次我们都猜是sea bass或者salmon，得到的结果一定不尽人意，毕竟我们知道，两种鱼都有可能出现，随着实验次数的增加，我们猜错的可能性是$P(sea\ bass)$和$P(salmon)$中更小的一个。</p><p>通常情况下，我们做决策的依据可能不止一个，即我们会有更多特征来辅助决策。例如我们还知道鱼的光泽度$x$，假设$x$是一个随机连续变量，那么我们的猜测策略就要从刚才的纯根据先验概率升级为根据条件概率进一步猜测。想象一个场景，随机摸一条鱼，然后有人跟我们说，它的光泽度是5，那么我们会想，光泽度为5时，是sea bass的可能性大呢，还是salmon的可能性大呢？</p><p>那么我们就需要计算$p(sea\ bass|x=5)$和$p(salmon|x=5)$，比较这两个值，取大的做猜测结果。</p><p>想想我们已知的有哪些？首先我们知道，$P(sea\ bass)$和$P(salmon)$, 其次我们知道每种鱼的光泽度分布，即$p(x|sea\ bass)$和$p(x|salmon)$。</p><p>ok，现在根据已知计算$p(sea\ bass|x)$和$p(salmon|x)$。以$p(sea bass|x)$为例：</p><ul><li><p>(1): $p(sea\ bass|x) = p(sea\ bass\ \&amp;\ x) / p(x)$</p></li><li><p>(2): $p(x|sea\ bass) = p(x\ \&amp;\ sea\ bass) / P(sea\ bass)$</p></li><li><p>(3): 由(2)得：$$p(sea\ bass\ \&amp;\ x) =  p(x|sea\ bass) * P(sea\ bass)$$</p></li><li><p>(4): 把(3)带入(1)得：$$p(sea\ bass|x) = p(x|sea\ bass) * P(sea\ bass) / p(x)$$</p></li></ul><p>使用上述方法就可以计算出p(sea bass|x)的概率，p(sea bass|x)同理，如此就可以给出x=5时候的猜测。<br>上述计算公式(4)就是<strong>贝叶斯公式</strong>，通用写法为：<br>$$P\left(\omega_{j} | x \right) = \frac{p\left(x|\omega_{j}\right)P\left(\omega_{j}\right)}{p\left(x\right)}$$</p><p>其中$\omega$代表某个类，$x$代表某个特征。</p><p>如果我们猜是sea bass，可想而知，我们的错误率是$p(salmon|x)$,否则错误率是$p(sea \ bass|x)$。用公式表达为<br>$$<br>P(error | x)=<br>\begin{cases}<br>P(salmon|x), &amp;\text{if we decide sea bass} &amp;\<br>P(sea \ bass|x), &amp;\text{if we decide salmon}<br>\end{cases}<br>$$</p><p>即$P(error | x)=min[P(sea\ bass|x),P(salmon|x)]$</p><h2 id="拓展"><a href="#拓展" class="headerlink" title="拓展"></a>拓展</h2><p>上面是一个简单的例子，在实际情况中，往往不止一个特征，不止一个类，决策（action）也不止是要分到某个类（比如发现两个分类的概率一样时可以拒绝决策），使用错误率计算代价可能太粗略，或许需要每一个action都计算一下损失函数，因此还需要在上面的基础上进行拓展。</p><p>我们用$\omega_{1},\omega_{2},…,\omega_{c}$表示事件的$c$个类别，用$\alpha_{1}, \alpha_{2}, …,\alpha_{a}$表示可以采取的a个决策(action)。当类别为$\omega_{j}$，行动为$\alpha_{i}$时，损失函数为$\lambda\left(\alpha_{i}|\omega_{j}\right)$。特征向量$x$表示一个$d$维的随机向量，$p\left(x|\omega_{j}\right)$表示$x$的条件密度函数。$P(\omega_{j})$表示$\omega_{j}$的先验概率。则后验概率可以用贝叶斯公式计算出来：<br>$$P\left(\omega_{j} | x \right) = \frac{p\left(x|\omega_{j}\right)P\left(\omega_{j}\right)}{p\left(x\right)}$$<br>其中，$p(x)$为：<br>$$<br>p(x)=\sum^c_{j=1}p(x|\omega_j)P(\omega_j)<br>$$</p><p>假设我们针对一个具体的特征$x$,采取行动$\alpha_i$,实际类别是$\omega_j$,则损失函数为$\lambda(\alpha_i|\omega_j)$,后验概率为$P(\omega_j|x)$, 在决策理论的术语中，可预期的损失叫做风险$risk$，$R(\alpha_i|x)$叫做条件风险$conditional\ risk$，则条件风险为<br>$$<br>R(\alpha_i|x)=\sum^c_{j=1}\lambda(\alpha_i|\omega_j)P(\omega_j|x)<br>$$<br>为了得到好的结果，我们需要最小化整体风险，即每次都采取条件风险最小的$action$，最终使整体风险最小化。</p><p>上述说法用数学如何表示呢？“每次采取条件风险最小的$action$”描述了一个决策规则($decision\ rule$),这个规则可以用函数$\alpha(x)$表示，那么特征$x$的条件风险就是$R(\alpha(x)|x)$，特征$x$的概率是$p(x)$，可想而知，整体风险就是<br>$$<br>R=\int R(\alpha(x)|x)p(x)\ dx<br>$$<br>我们想让整体风险最小化，在计算中只要让每个条件风险最小化即可。最小化的整体风险被称为贝叶斯风险$Bayes\ risk$，是能达到的最佳表现。<br></p><h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><ul><li>MIT课程指定的Reading：Duda, Richard O., Peter E. Hart, and David G. Stork. <em>Pattern Classification</em>. New York, NY: John Wiley &amp; Sons, 2000. ISBN: 9780471056690.</li></ul>]]></content>
      
      <categories>
          
          <category> MIT </category>
          
          <category> Pattern Recognition </category>
          
      </categories>
      
      
        <tags>
            
            <tag> MIT课程 </tag>
            
            <tag> 模式识别 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>模式识别与分析——贝叶斯决策理论（数学部分）</title>
      <link href="/2018/06/07/MIT-PatternRecognition2/"/>
      <url>/2018/06/07/MIT-PatternRecognition2/</url>
      <content type="html"><![CDATA[<h1 id="贝叶斯决策理论相关数学知识"><a href="#贝叶斯决策理论相关数学知识" class="headerlink" title="贝叶斯决策理论相关数学知识"></a>贝叶斯决策理论相关数学知识</h1><p>主要是概率论，如果你这部分基础牢固，可以跳过，直接看理论部分。</p><h2 id="概率质量函数"><a href="#概率质量函数" class="headerlink" title="概率质量函数"></a>概率质量函数</h2><p>概率质量函数（Probability Mass Function）是针对离散值而言的，通常用大写字母P表示。假设某个事<br><a id="more"></a><br>件$\omega_{1}$发生的概率为$P(\omega_{1})$,某个事件$\omega_{2}$发生的概率为$P(\omega_{2})$,两事件相互独立，则$P(\omega_{1})+P(\omega_{2})=1$。</p><h2 id="概率密度函数"><a href="#概率密度函数" class="headerlink" title="概率密度函数"></a>概率密度函数</h2><p>概率密度函数（Probability Desity Function）是针对连续值而言的，通常用小写字母$p$表示。<strong>概率密度函数的在正无穷到负无穷上到积分为1</strong>，在某一个区间中的概率用在该区间中的积分来表示。</p><p>用数学语言描述就是：</p><p>$<br>(1)p(\overrightarrow {x}) \geq 0, \forall\ \overrightarrow {x}\in R^n<br>$<br>$<br>(2)\int p(\overrightarrow {x})\ d\ \overrightarrow x=1<br>$</p><p><strong>NOTE</strong>：</p><ul><li>$\overrightarrow x$是一个列向量</li></ul><p>任何满足以上两个条件的函数都叫在n为<a href="https://en.wikipedia.org/wiki/Euclidean_space" target="_blank" rel="noopener">欧几里得空间</a>（Euclidean Space）上的概率密度函数。</p><p>比如：</p><h5 id="高斯密度函数（Gaussian-Density-Function-or-Density-Function-for-Gaussian-Distribution）"><a href="#高斯密度函数（Gaussian-Density-Function-or-Density-Function-for-Gaussian-Distribution）" class="headerlink" title="高斯密度函数（Gaussian Density Function or Density Function for Gaussian Distribution）"></a>高斯密度函数（Gaussian Density Function or Density Function for Gaussian Distribution）</h5><p>高斯密度函数的定义为：</p><p>$$<br>p(\overrightarrow x)=\dfrac{1}{(\sqrt{2\pi})^n|\Sigma|^{1/2}}exp\left{-\dfrac{1}{2}(\overrightarrow x-\overrightarrow \mu)^T\Sigma^{-1}(\overrightarrow x - \overrightarrow \mu)\right}<br>$$<br><strong>NOTE：</strong></p><ul><li><p>可能发现上面那个公式和平时见的公式长得不太一样，其实它是从线性代数的角度写的。</p></li><li><p>公式中的$|\Sigma|$代表Determinant of sigma,  也就是$\Sigma$的行列式，将nxn的矩阵映射成一个标量（既然提到了行列式并且我也有些遗忘，所以一会儿在文末附录里整理一下它的概念）。$\Sigma$是什么呢？它叫Variance-Covariance Matrix， 也叫Dispersion Matrix，是一个nxn的矩阵，它的逆$\Sigma^{-1}$也是一个nxn的矩阵。（这里协方差矩阵和矩阵的逆还有矩阵的转置，也要在附录里温习）ok，回归正题，这个determinant of sigma可能是0也可能是负数，但是如果是负数，1/2次方就会很难计算，因为它会得到一个非常复杂的数， 而我们的概率密度函数的第一个条件就是$p(\overrightarrow x)\geq0$，<strong>所以determinant ofsigma必须大于0</strong>， 因为即使是等于0，1/0也无法计算。</p></li><li>$exp$代表e的某次方。</li><li>$\overrightarrow x$：一个$n$维的向量</li><li>$\overrightarrow \mu$：均值向量，代表分布的均值，也是一个$n$维的向量（mean vector同样在附录里温习）</li><li><p>因为$\overrightarrow x$和$\overrightarrow \mu$都是n维的列向量，所以$(\overrightarrow x-\overrightarrow \mu)$也是一个n维的列向量，即nx1的矩阵，所以$(\overrightarrow x-\overrightarrow \mu)^T$是一个n维的行向量， 即1xn的矩阵</p></li><li><p>所以$(\overrightarrow x-\overrightarrow \mu)^T\Sigma^{-1}(\overrightarrow x - \overrightarrow \mu)$是一个标量，所以这一项是e的任何大于等于0的次方。</p></li></ul><p>看完这里，请<strong>跳到附录</strong>，补充<strong>Variance-Covariance Matrix和Positive-Definite Matrix</strong> 的概念，至于行列式（Determinant）和矩阵的逆以及矩阵的转置，看不看都行。</p><h2 id="先验概率"><a href="#先验概率" class="headerlink" title="先验概率"></a>先验概率</h2><p>先验概率（Prior Probability）是指根据已有情况提前知道的概率，比如已知有一箱红黑混合的小球，其中红色小球共有100颗，黑色小球共有200颗，则红色小球的先验概率为$P(red) = 1/3$, 黑色小球的鲜艳概率为$P(black) = 2/3$。</p><h2 id="条件概率"><a href="#条件概率" class="headerlink" title="条件概率"></a>条件概率</h2><p>假设将上述红黑混合的小球们放在两个箱子中，即A箱放20个红色小球，100个黑色小球，B箱放80个红色小球，100个黑色小球，则从A中取到红色小球的概率是多少？这就是条件概率。</p><p>$$P(red|A) = P(red \&amp; A) / P(A) = (20 / 300) / (120 / 300) = 1/6$$</p><p>那么，红色里面来自A的概率是多少呢？</p><p>$$P(A|red) = P(A \&amp; red) / P(red) = (20 / 300) / (100 / 300) =  1 / 5$$</p><h1 id="附录"><a href="#附录" class="headerlink" title="附录"></a>附录</h1><h2 id="Variance-Covariance-Matrix"><a href="#Variance-Covariance-Matrix" class="headerlink" title="Variance-Covariance Matrix"></a>Variance-Covariance Matrix</h2><p>首先需要知道Variance和Covariance的定义。</p><h4 id="Variance"><a href="#Variance" class="headerlink" title="Variance"></a>Variance</h4><p>假设有n个observations：$$x_1, x_2, x_3, …, x_n \in R$$</p><p>它们的平均数$\bar x$等于:<br>$$<br>\bar x=\dfrac {1}{n}\sum_{i=1}^{n}x_i<br>$$<br>它们的方差Variance等于<br>$$<br>Variance=\dfrac {1}{n}\sum_{i=1}^{n}(x_i-\bar x)^2<br>$$</p><blockquote><p>一些书也会写为：$<br>Variance=\dfrac {1}{n-1}\sum_{i=1}^{n}(x_i-\bar x)^2$，这实际上是unbias estimate for Variance of the population,与1/n在value上有些差别，这是统计学中比较复杂的一个概念。（老师没有做详细介绍，说可以课后去查，而我也不打算深入此概念，所以和老师一样，variance就follow第一种写法。）</p></blockquote><h4 id="Covariance"><a href="#Covariance" class="headerlink" title="Covariance"></a>Covariance</h4><p>为了阐释协方差Covariance，我们需要两个变量(x, y)，假设x是身高，单位是cm，y是体重，单位是kg。<br>假设有n个observations：<br>$$(x_1,y_1),(x_2, y_2), …, (x_n, y_n)$$<br>你要做的是plot these points，这里我给出三个这样的plots，灰色区域是一些列点：<br><img src="https://upload-images.jianshu.io/upload_images/4964755-6c0ef0e6436e4d1b.JPG?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="plot the points"><br>图（1）中，x增长，y随x也增长，所以我们用一些大于0的数（quantity）来代表这个关系；<br>图（2）中，x增长，y随x减小，我们用一些小于0的数（quantity）来代表这个关系；<br>图（3）中，x增长，y在某一个范围内波动，所以我们用一些非常接近0的数（quantity）来代表这个关系。</p><p>那么这个数（quantity）到底是什么呢？</p><p>对于所有的x和y，我们找到它们的均值，然后将其作为新坐标轴的原点：<br><img src="https://upload-images.jianshu.io/upload_images/4964755-4fa3427c50436d5a.JPG?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="new axis"><br>那么所有点的x，y值都会变化，把这些新的值乘起来求均值，会得到什么呢？</p><p>比如图（1），新坐标系第一象限的x，y都大于0，乘积也会大于0，第三象限x，y都小于0，乘积也会大于0，第二和第四象限乘积会小于0，但是一三象限的点数量明显大于二四象限的点，所以我们计算<br>$$<br>\dfrac{1}{n}\sum_{i=1}^n(x_i - \bar x)(y_i - \bar y)<br>$$<br>会得到一个大于0的值。</p><p>同理图（2）会得到一个小于0的值，图（3）会得到一个约等于0的值。</p><p>这就是x和y的协方差Covariance<br>$$<br>Cov(x,y)=\dfrac{1}{n}\sum_{i=1}^n(x_i - \bar x)(y_i - \bar y)<br>$$<br>可以看出，$Cov(x,x)$就是variance。</p><h4 id="Variance-Covariance-Matrix-1"><a href="#Variance-Covariance-Matrix-1" class="headerlink" title="Variance-Covariance Matrix"></a>Variance-Covariance Matrix</h4><p>在模式识别中，我们把这一系列变量称作features，如果两两组合，会得到多少对呢？$n^2$对。<br>如果n个features是<br>$$<br>x_1, x_2, x_3, \dots,x_n<br>$$<br>则这n个features的Variance-Covariance matrix为：<br>$$<br>\Sigma=\begin{bmatrix}<br>{Cov(x_1,x_1)}&amp;{Cov(x_1, x_2)}&amp;{\cdots}&amp;{Cov(x_1, x_n)}\<br>{Cov(x_2,x_1)}&amp;{Cov(x_2,x_2)}&amp;{\cdots}&amp;{Cov(x_2,x_n)}\<br>{\vdots}&amp;{\vdots}&amp;{\ddots}&amp;{\vdots}\<br>{Cov(x_n,x_1)}&amp;{Cov(x_n,x_2)}&amp;{\cdots}&amp;{Cov(x_n,x_n)}\<br>\end{bmatrix}$$<br>这是一个对称矩阵symmetric matrix，也是一个正定矩阵<a href="https://en.wikipedia.org/wiki/Positive-definite_matrix" target="_blank" rel="noopener">Positive-definite matrix</a>，什么是正定矩阵呢，往下看。</p><h2 id="Positive-definit-matrix"><a href="#Positive-definit-matrix" class="headerlink" title="Positive-definit matrix"></a>Positive-definit matrix</h2><p>大家应该都知道“欧几里得”距离是什么吧，假设我们有一个列向量$\overrightarrow x=[x_1, x_2, \dots,x_n]$和一个列向量$\overrightarrow y=[y_1, y_2, \dots, y_3]$,则x和y的欧几里得距离为$d(\overrightarrow x, \overrightarrow y)=\sqrt{\sum_{i=1}^n(x_i-y_i)^2}$。</p><p>现在假设x代表第一个人的feature，y代表第二个人的feature，每个列向量只有两列，分别代表身高和体重。</p><p>x的身高和体重分别为160cm和70kg，y的身高和体重分别为158cm和73kg，现在想衡量x和y的距离，如果用上面的欧式距离，就会有些问题，为什么这么说呢？</p><p>$$d(\overrightarrow x, \overrightarrow y)=\sqrt{(160-158)^2+(70-73)^2}=\sqrt{13}$$</p><p>如果同样是这两个人，把身高的单位换成mm，同样的方式计算x和有的距离：<br>$$d(\overrightarrow x, \overrightarrow y)=\sqrt{(1600-1580)^2+(70-73)^2}=\sqrt{409}$$<br>这是我们不期望得到的结果，相同的两个人，衡量他们的距离，应该无论如何都始终一样，而非仅仅换了单位就出现不同的结果。</p><p>所以欧式距离往往是not useful的。</p><p>现在让我们移除公式的根号：<br>$$d^2(\overrightarrow x, \overrightarrow y)=(x_1-y_1, x_2-y_2)\left(\begin{array}{cccc}<br>    1 &amp;    0 \<br>    0 &amp;    1\<br>\end{array}\right) \left(\begin{array}{cccc}<br>    x_1-y_1 \<br>    x_2-y_2 \<br>\end{array}\right)<br>$$<br>这种写法与$d(\overrightarrow x, \overrightarrow y)=\sqrt{\sum_{i=1}^2(x_i-y_i)^2}$是等价的，一个矩阵与单位矩阵（identity matrix）相乘是不变的。</p><p>现在我们对中间对单位矩阵做一些泛化，把它改成$\left(\begin{array}{cccc}<br>    w_1 &amp;    0 \<br>    0 &amp;    w_2\<br>\end{array}\right)$，则相应的距离公式变为$d(\overrightarrow x, \overrightarrow y)=\sqrt{\sum_{i=1}^2w_i(x_i-y_i)^2}$，这里的$w_i$取决于单位，这样就能解决我们的问题：当单位发生改变时，相同的两个人距离不发生改变。这种定义下，如果$w_1$和$w_2$严格大于等于0，那么最后的<strong>距离就是大于或者等于0的</strong></p><p>你还可以继续泛化，把单位矩阵改成$\left(\begin{array}{cccc}<br>    w_{11} &amp;    w_{12} \<br>    w_{21} &amp;    w_{22} \<br>\end{array}\right)$，在这种定义下，w的值该取多少距离才会大于0呢？<br>满足这样的w有很多，比如随便举个例子：<br>$$<br>(x_1-y_1, x_2-y_2)\left(\begin{array}{cccc}<br>    2 &amp;    -1 \<br>    -1 &amp;    2 \<br>\end{array}\right)\left(\begin{array}{cccc}<br>    x_1-y_1 \<br>    x_2-y_2\<br>\end{array}\right) &gt; 0, \ if(x_1 \neq y_1)\ or\ (x_2 \neq y_2)<br>$$</p><p>这样我们的Positive-definite matrix就有了定义：</p><blockquote><p>$A_{n<em>n}$ is said to be POSITIVE DEFINITE if $a^TAa &gt; 0, \ \forall a \neq \left(\begin{array}{cccc}<br>    0 \<br>    0 \<br>    0 \<br>    . \<br>    . \<br>    . \<br>    0<br>\end{array}\right)_{n</em>1}$</p></blockquote><p>如果：</p><blockquote><p>$$a^TAa \geq 0, \ \forall a. $$ $A_{n*n}$ is said to be POSITIVE SEMI DEFINITE, in some books, this is also written as NON NEGATIVE DEFINITE.</p></blockquote><p>在线性代数中，很容易找到positive-definite matrix的定义，那么我们为什么需要一个这样的矩阵呢？从上面那个“距离”的角度来说，我们需要这样的矩阵是因为我们要确保距离是大于等于0的，如果x和y是完全一样的，我们希望其距离为0，否则我们希望一个大于0的数来表示不同程度，因此我们把距离公式写成$a^TAa$的矩阵形式。</p><p>Variance-Covariance matrix  可以被证明是NON-NEGATIVE DEFINITE的，实际上通常是POSITIVE-DEFINITE的，在正态分布（高斯密度函数）下，我们认为Variance-Covariance matrix 是POSITIVE-DEFINITE的，这就是我们为什么会在高斯密度函数的分母上把它写成$|\Sigma|^{1/2}$的原因。如果Variance-Covariance matrix是non-negative definite的，就会有一些properties，比如矩阵的行列式是它的特征值（the determinant of matrix is product of its eigenvalues），所有的特征值都是大于等于0的，如果variance-covariance matrix是positive-definite的，那么所有eigenvalues都是严格大于0的，所以可以把它做分母。</p><h2 id="Determinant"><a href="#Determinant" class="headerlink" title="Determinant"></a>Determinant</h2><p>在线性代数里，Determinant是一个可以从方形矩阵中计算出来的值。矩阵的Determinant记做$det(A)\ or \ detA \ or \ |A|$。在几何学里，它被视作描述矩阵线性变换的scaling factor。</p><p>2x2的矩阵行列式计算方法为：<br>$$<br>|A|=\left|\begin{array}{cccc}<br>    a &amp; b  \<br>    c &amp; d \<br>\end{array}\right|=ad - bc<br>$$<br>更高阶的计算方法到参考文献的链接查看吧，mathjax不好写了。</p><h2 id="Matrix-inverse"><a href="#Matrix-inverse" class="headerlink" title="Matrix inverse"></a>Matrix inverse</h2><p>在线性代数中，如果一个nxn的方阵A存在一个nxn的方阵B使其满足<br>$$AB=BA=I_n$$<br>则称A为可逆矩阵，B是A的逆。$I_n$是nxn的Identity Matrix，也被含糊地称为Unit Matrix，单位矩阵，对角线是1，其余是0。</p><h2 id="Transpose-of-Matrix"><a href="#Transpose-of-Matrix" class="headerlink" title="Transpose of Matrix"></a>Transpose of Matrix</h2><p>在线性代数里，矩阵的转置就是行列元素的索引对调，记做$A^T$:<br>$$<br>[A^T]<em>{ij}=[A]</em>{ji}<br>$$</p><h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><ul><li><a href="https://www.youtube.com/watch?v=mfePdDh9t6Q&amp;list=PLbMVogVj5nJQJMLb2CYw9rry0d5s0TQRp" target="_blank" rel="noopener">印度的讲的贼好的MOOC</a></li><li><a href="https://en.wikipedia.org/wiki/Determinant" target="_blank" rel="noopener">矩阵行列式（Determinant）</a></li><li><a href="https://en.wikipedia.org/wiki/Invertible_matrix" target="_blank" rel="noopener">矩阵的逆（Inverse）</a></li><li><a href="https://en.wikipedia.org/wiki/Transpose" target="_blank" rel="noopener">矩阵的转置</a></li></ul>]]></content>
      
      <categories>
          
          <category> MIT </category>
          
          <category> Pattern Recognition </category>
          
      </categories>
      
      
        <tags>
            
            <tag> MIT课程 </tag>
            
            <tag> 模式识别 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>模式识别与分析——简介及大纲</title>
      <link href="/2018/06/06/MIT-PatternRecognition/"/>
      <url>/2018/06/06/MIT-PatternRecognition/</url>
      <content type="html"><![CDATA[<p>越往后研究，越发现数学是瓶颈，很多模型只停留在会用的阶段，稍遇到棘手的问题就发现自己现有的学识是多么浅薄，所以决定放下手头亟待解决的问题，静下来磨刀练剑。找到2006年秋季<a href="https://ocw.mit.edu/courses/media-arts-and-sciences/mas-622j-pattern-recognition-and-analysis-fall-2006/index.htm" target="_blank" rel="noopener">MIT的一门课程</a>，英文名叫<em>Pattern Recognition and Analysis</em>，课程只给了Readings，是<em>Pattern Classification</em>，作者是Duda，结合一个印度的<a href="https://www.youtube.com/watch?v=mfePdDh9t6Q&amp;list=PLbMVogVj5nJQJMLb2CYw9rry0d5s0TQRp" target="_blank" rel="noopener">讲的很好的MOOC</a>，重新学习一下模式识别，在这里记下学习笔记（注意，这些文字只是我一段时光的记录，<strong>不是教程！不是教程！不是教程！</strong>）。当然，如果碰巧你也在学习这部分知识，可以一起探讨前行，感谢指出问题与不足。<br><a id="more"></a></p><p>每个知识点的学习思路包含以下三个部分：</p><ul><li>数学部分（与知识点紧密相关的数学知识）</li><li>理论部分（知识点的详细介绍）</li><li>实战部分（知识点对应的代码实现，如果我找到合适的项目应用，也会一并写出来，开源）</li></ul><h1 id="MIT的大纲"><a href="#MIT的大纲" class="headerlink" title="MIT的大纲"></a>MIT的大纲</h1><p>不出意外的话我的学习线路也会跟着这个大纲。</p><p>这门课涉及了表征和识别数字数据中的模式及特征的基础知识，讨论了用户建模(user modeling), 影响识别(affect recognition), 语音识别(speech recognition), 计算机视觉(computer vision), 生理分析(physiological analysis)等应用等基本工具和理论，覆盖了决策理论(decision theory)， 统计分类(statistical classification)，最大似然(maximum likelihood)和贝叶斯估计(Bayesian estimation)， 非参数方法(nonparametic methods)， 无监督学习(unsupervised learning)和聚类(clustering)，还包含了一些来自active reaserch等机器和人类学习相关话题。</p><ul><li>介绍模式识别(Pattern Recognition)，特征检测(Feature Detection)，分类(classification)</li><li>综述概率论(Probability Theory)，条件概率(Conditional Probability)和贝叶斯规则(Bayes Rule)</li><li>随机向量(Random Vectors)，期望(Expectation)，相关(Correlation)，协方差(Covariance)</li><li>综述线性代数(Linear Algebra)，线性变换(Linear Transformations)</li><li>决策理论(Decision Theory)， ROC曲线(ROC Curves), 似然比检验(likelihood Ratio Test)</li><li>线性和二次判别式(Linear and Quadratic Discriminants)，Fisher判别式(Fisher Discriminate)</li><li>充分统计量(Sufficient Statistics)，应对缺失或噪声特征</li><li>基于模版等识别(Template-based Recognition)，特征提取(Feature Extraction)</li><li>特征向量(Eigenvector)与多重线性分析(Multilinear Analysis)</li><li>训练方法，最大似然和贝叶斯参数估计</li><li>线性判别/感知器学习，梯度下降优化</li><li>支持向量机</li><li>K-近邻分类</li><li>非参数分类，密度估计，Parzen估计</li><li>无监督学习，聚类，矢量量化(Vector Quantization)， K-Means</li><li>混合建模， 期望最大化</li><li>隐马尔可夫模型， Viterbi算法， Baum-Welch算法</li><li>线性动态系统，卡尔曼滤波</li><li>贝叶斯网络</li><li>决策树， 多层感知器</li><li>人机交互中的强化学习</li><li>遗传算法(Genetic Algorithms)</li><li>多分类器“Committee Machines”的组合</li></ul>]]></content>
      
      <categories>
          
          <category> MIT </category>
          
          <category> Pattern Recognition </category>
          
      </categories>
      
      
        <tags>
            
            <tag> MIT课程 </tag>
            
            <tag> 模式识别 </tag>
            
        </tags>
      
    </entry>
    
  
  
    
    <entry>
      <title>about</title>
      <link href="/about/index.html"/>
      <url>/about/index.html</url>
      <content type="html"><![CDATA[<p></p>]]></content>
    </entry>
    
    <entry>
      <title>categories</title>
      <link href="/categories/index.html"/>
      <url>/categories/index.html</url>
      <content type="html"><![CDATA[]]></content>
    </entry>
    
    <entry>
      <title>tags</title>
      <link href="/tags/index.html"/>
      <url>/tags/index.html</url>
      <content type="html"><![CDATA[]]></content>
    </entry>
    
  
</search>
